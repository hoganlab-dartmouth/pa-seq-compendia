# Automatically add new SRX accessions to the original compendia

This folder contains an automated snakemake pipeline that adds new SRX accessions to the original compendia.

## Getting started with this pipeline

### Installation and software

To get started, clone the repository to the computer you would like to run it on and then navigate into this folder in the repository.
```
git clone XXX # update with clone url if merged back into hogan lab org
cd pa-seq-compendia/auto_add_to_compendia
```
 
This pipeline uses conda to manage software installations. 
You can find operating system-specific instructions for installing miniconda [here](https://docs.conda.io/en/latest/miniconda.html).
To setup the environment required for this repository, run the following commands:
```
conda env create --name compendia --file environment.yml
conda activate compendia
```

### Specifying samples

The pipeline requires a metadata file that specifies the SRX accessions to add to the compendia.
The file must be named `metadata.tsv` and must contain the columns `run_accession`, `experiment_accession`, and `fastq_ftp`.
We have provided an example metadata file above.
The easiest way to generate new metadata files is to use the [European Nucleotide Archive](https://www.ebi.ac.uk/ena/browser/home).
After searching for the accession(s) or studies you would like to add, export a report as a TSV, download and rename it to `metadata.tsv` and place it in this folder.

![](https://i.imgur.com/HOcP1SN.png)
  
Make sure `run_accession`, `experiment_accession`, and `fastq_ftp` are selected before downloading the table.

### Running the pipeline with snakemake

The pipeline is run using snakemake.
Snakemake can parallelize job submission and modulate resource usage (RAM, CPUs). 
We used the command below in a slurm cluster, but other cluster engines [are also supported](https://snakemake.readthedocs.io/en/stable/executing/cluster.html).

```
snakemake -s add_sra_to_compendia.snakefile -j 16 --use-conda --rerun-incomplete --latency-wait 15 -
-resources mem_mb=200000 --cluster "sbatch -t 720 -J comp -p bmm -n 1 -N 1 -c {threads} --mem={resources.mem_mb}" -k
```

Alternatively, snakemake can be executed without a cluster:
```
snakemake -s add_sra_to_compendia.snakefile -j 2 --use-conda --rerun-incomplete -k -n
```
These parameters are described below:

+`-s` specifies the snakefile to be executed (here, `add_sra_to_compendia.snakefile`)
+`-j 2` parallelizes the snakefile over two cores (drop to `-j 1` to only run one process at a time)
+ `--use-conda` tells snakemake to use conda to manage software environments for each rule
+ `--rerun-incomplete` tells snakemake to rerun rules when it thinks a file is incomplete, e.g. as may occur if a file is half-finished when a snakemake process is terminated.
+ `-k` indicates for the snakefile to keep running even if a rule fails. Snakemake will attempt to run all rules that don't depend on the output of the failed rule.
+ `-n` specifies a dry run. Remove this to actually execute the snakefile. The dry run is useful to make sure snakemake is running the desired rules for the desired number of times.

## Pipeline outputs

All outputs generated by the compendia will be written to an `outputs` folder.
The new compendia will be located at these file names:
```
# Filtered compendia, with raw NumReads as reported by salmon
outputs/filt_norm_compendia/pao1_aligned_compendium_p2_filtered_num_reads.csv",
outputs/filt_norm_compendia/pa14_aligned_compendium_p2_filtered_num_reads.csv",
# Filtered compendia, with transcripts per million (TPM) as reported by salmon
outputs/filt_norm_compendia/pa14_aligned_compendium_p2_filtered_tpm.csv",
outputs/filt_norm_compendia/pao1_aligned_compendium_p2_filtered_tpm.csv",
# Filtered and normalized compendia
outputs/filt_norm_compendia/pa14_aligned_compendium_p2_filtered_counts_norm.csv",
outputs/filt_norm_compendia/pao1_aligned_compendium_p2_filtered_counts_norm.csv",
```
